{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/dl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda2/envs/dl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "img_path=\"data/images/\"\n",
    "img_height=224 #299\n",
    "img_width=224 #299\n",
    "\n",
    "# credit: https://github.com/fastai/fastai/blob/9e9ffbd49eb6490bb1168ce2ff32b10a81498ba9/fastai/utils.py\n",
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]\n",
    "\n",
    "def plot_img(img, title, count, cols, plot_axis=False):\n",
    "    a = fig.add_subplot(1, cols, count)\n",
    "    # if 'img' is a NumPy array, then it has already been loaded; just show it\n",
    "    if type(img).__module__ == np.__name__:\n",
    "        plt.imshow(img)\n",
    "    else:\n",
    "        plt.imshow(load_img(img))\n",
    "    a.set_title(title,fontsize=10)\n",
    "    if plot_axis is False:\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/images/\n",
      "['Fibrosis', 'ALL']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def create_label_directories(csv_filename, img_path, is_one_v_all=False, one_v_all_label=\"Fibrosis\"):\n",
    "    directories = set()\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(csvfile) # skip header row\n",
    "        for row in reader:\n",
    "            img_filename = str(row[0])\n",
    "            labels = str(row[1])\n",
    "            for label in labels.split('|'):\n",
    "                if (is_one_v_all is True) and (label != one_v_all_label):\n",
    "                    label = \"ALL\"\n",
    "                src_file = os.path.join(img_path,img_filename)\n",
    "                label = \"_\".join(label.split())\n",
    "                dst_train_dir = os.path.join(img_path,\"train\",label)\n",
    "                dst_train_file = os.path.join(dst_train_dir,img_filename)\n",
    "                dst_valid_dir = os.path.join(img_path,\"valid\",label)\n",
    "                dst_test_dir = os.path.join(img_path,\"test\",label)\n",
    "                if not os.path.exists(dst_train_dir):\n",
    "                    os.makedirs(dst_train_dir)\n",
    "                    directories.add(label)\n",
    "                if not os.path.exists(dst_valid_dir):\n",
    "                    os.makedirs(dst_valid_dir)\n",
    "                if not os.path.exists(dst_test_dir):\n",
    "                    os.makedirs(dst_test_dir)\n",
    "                src_file_abs = os.path.join(os.getcwd(),src_file)\n",
    "                dst_train_file_abs = os.path.join(os.getcwd(),dst_train_file)\n",
    "                #print(\"copy: \" + src_file_abs + \" to: \" + dst_train_file_abs)\n",
    "                if not os.path.exists(dst_train_file_abs):\n",
    "                    os.symlink(src_file_abs, dst_train_file_abs)\n",
    "    return list(directories)\n",
    "\n",
    "is_one_v_all = True\n",
    "one_v_all_label = \"Fibrosis\"\n",
    "print(img_path)\n",
    "directories = create_label_directories(\"data/Data_Entry_2017.csv\", img_path, is_one_v_all, one_v_all_label)\n",
    "print(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fibrosis', 'ALL']\n",
      "[1686, 111393]\n"
     ]
    }
   ],
   "source": [
    "def get_per_label_count(directories):\n",
    "    per_label_count = []\n",
    "    for ii in range(len(directories)):\n",
    "        #print(directories[ii])\n",
    "        path, dirs, files = os.walk(os.path.join(img_path,\"train\",directories[ii])).__next__()\n",
    "        file_count = len(files)\n",
    "        per_label_count.append(file_count)\n",
    "    return per_label_count\n",
    "        \n",
    "print(directories)\n",
    "per_label_count = get_per_label_count(directories) \n",
    "print(per_label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fibrosis', 'ALL']\n",
      "[1686, 111393]\n",
      "[8430, 1686]\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def upsample(directories, per_label_count, iqr):\n",
    "    for ii in range(len(per_label_count)):\n",
    "        label = directories[ii]\n",
    "        count = per_label_count[ii]\n",
    "        if count < iqr:\n",
    "            offset = iqr-count\n",
    "            subprocess.call(['./batch-augment.sh', os.path.join(os.getcwd(),img_path,\"train\",label), str(offset)])\n",
    "    return get_per_label_count(directories)\n",
    "\n",
    "print(directories)\n",
    "print(per_label_count)\n",
    "#print(label_batch_size)\n",
    "per_label_count_upsampled = upsample(directories, per_label_count, 8430)#label_batch_size)\n",
    "print(per_label_count_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fibrosis', 'ALL']\n",
      "[8430, 111393]\n",
      "[8430, 8430]\n"
     ]
    }
   ],
   "source": [
    "def downsample(directories, per_label_count):\n",
    "    label_idx = np.argmin(per_label_count)\n",
    "    #print(label_idx)\n",
    "    downsample_count = per_label_count[label_idx]\n",
    "    #print(downsample_count)\n",
    "    for ii in range(len(per_label_count)):\n",
    "        label = directories[ii]\n",
    "        src_train_dir = os.path.join(img_path,\"train\",label)\n",
    "        all_img_paths = glob.glob(os.path.join(src_train_dir,\"*.*\"))\n",
    "        np.random.shuffle(all_img_paths)\n",
    "        if len(all_img_paths) != downsample_count:\n",
    "            imgs_to_remove = all_img_paths[downsample_count:]\n",
    "            #print(len(imgs_to_remove))\n",
    "            for file in imgs_to_remove:\n",
    "                file_abs = os.path.join(os.getcwd(),file)\n",
    "                #print(\"remove file: \" + file_abs)\n",
    "                os.remove(file_abs)\n",
    "    return get_per_label_count(directories)\n",
    "                \n",
    "print(directories)\n",
    "print(per_label_count_upsampled)\n",
    "per_label_count_downsampled = downsample(directories, per_label_count_upsampled)\n",
    "print(per_label_count_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fibrosis', 'ALL']\n",
      "[8430, 8430]\n"
     ]
    }
   ],
   "source": [
    "def split_train_valid_test(directories, per_label_count, valid_pct, test_pct):\n",
    "    for ii in range(len(directories)):\n",
    "        all_img_paths = glob.glob(os.path.join(img_path,\"train\",directories[ii],\"*.*\"))\n",
    "        np.random.shuffle(all_img_paths)\n",
    "        label_count = per_label_count[ii]\n",
    "        valid_count = int(label_count*valid_pct)\n",
    "        valid_files = all_img_paths[:valid_count]\n",
    "        all_img_paths[:valid_count] = []\n",
    "        test_count = int(label_count*test_pct)\n",
    "        test_files = all_img_paths[:test_count]\n",
    "        all_img_paths[:test_count] = []\n",
    "        #print(len(valid_files))\n",
    "        #print(len(test_files))\n",
    "        train_files = all_img_paths\n",
    "        all_img_paths = []\n",
    "        #print(len(train_files))\n",
    "        for valid_file in valid_files:\n",
    "            valid_file_abs = os.path.join(os.getcwd(),valid_file)\n",
    "            #print(\"move: '\" + valid_file_abs + \"' to: '\" + os.path.join(img_path,\"valid\",directories[ii]))\n",
    "            shutil.move(valid_file_abs, os.path.join(img_path,\"valid\",directories[ii]))\n",
    "        for test_file in test_files:\n",
    "            test_file_abs = os.path.join(os.getcwd(),test_file)\n",
    "            #print(\"move: '\" + test_file_abs + \"' to: '\" + os.path.join(img_path,\"test\",directories[ii]))\n",
    "            shutil.move(test_file_abs, os.path.join(img_path,\"test\",directories[ii]))\n",
    "        \n",
    "\n",
    "valid_pct = 0.01 # 0.1\n",
    "test_pct = 0.98 # 0.1       \n",
    "#print(resampled_directories)\n",
    "#print(resampled_per_label_count)\n",
    "#split_train_valid_test(resampled_directories, resampled_per_label_count, valid_pct, test_pct)\n",
    "print(directories)\n",
    "print(per_label_count_downsampled)\n",
    "split_train_valid_test(directories, per_label_count_downsampled, valid_pct, test_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16188 images belonging to 2 classes.\n",
      "Found 336 images belonging to 2 classes.\n",
      "Found 336 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.densenet import preprocess_input\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #rescale=1./255,\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    #horizontal_flip=True, \n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/images/train',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "    #color_mode='grayscale')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'data/images/valid',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size, #val_batch_size,\n",
    "    class_mode='categorical')\n",
    "    \n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'data/images/test',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 1024)         0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 2)            2050        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,039,554\n",
      "Trainable params: 6,955,906\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "model = DenseNet121(include_top=True, weights=None, input_shape=(img_width,img_height,3), classes=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only missing -> cmd:option('-weightDecay', 1e-4, 'weight decay')\n",
    "from keras import optimizers\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "filepath=\"weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=500)\n",
    "#tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=64, write_graph=True, write_grads=False, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "callbacks_list = [checkpoint,early_stopping]#,tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1515\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.2267 - acc: 0.9160 - val_loss: 0.2029 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20285, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 2/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1878 - acc: 0.9336 - val_loss: 0.2074 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1863 - acc: 0.9365 - val_loss: 0.1761 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.20285 to 0.17610, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 4/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.2176 - acc: 0.9238 - val_loss: 0.1690 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17610 to 0.16902, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 5/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.2108 - acc: 0.9312 - val_loss: 0.1829 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1877 - acc: 0.9355 - val_loss: 0.1931 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1913 - acc: 0.9297 - val_loss: 0.2885 - val_acc: 0.9221\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1905 - acc: 0.9326 - val_loss: 0.2233 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1671 - acc: 0.9424 - val_loss: 0.2255 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.2066 - acc: 0.9238 - val_loss: 1.0411 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.2147 - acc: 0.9238 - val_loss: 0.2076 - val_acc: 0.9047\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.2097 - acc: 0.9258 - val_loss: 0.2278 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1629 - acc: 0.9463 - val_loss: 0.1892 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.2199 - acc: 0.9199 - val_loss: 0.3412 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1647 - acc: 0.9453 - val_loss: 0.4354 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1710 - acc: 0.9443 - val_loss: 0.1583 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.16902 to 0.15834, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 17/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1790 - acc: 0.9414 - val_loss: 0.1534 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.15834 to 0.15344, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 18/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1809 - acc: 0.9404 - val_loss: 0.2641 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1651 - acc: 0.9395 - val_loss: 0.2203 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1685 - acc: 0.9434 - val_loss: 0.1816 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1840 - acc: 0.9404 - val_loss: 0.1997 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1595 - acc: 0.9492 - val_loss: 0.1672 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1940 - acc: 0.9258 - val_loss: 0.1659 - val_acc: 0.9526\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1626 - acc: 0.9453 - val_loss: 0.3444 - val_acc: 0.8730\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1731 - acc: 0.9385 - val_loss: 0.1714 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1721 - acc: 0.9396 - val_loss: 0.2176 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1884 - acc: 0.9385 - val_loss: 0.3561 - val_acc: 0.9273\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1749 - acc: 0.9346 - val_loss: 0.4507 - val_acc: 0.8186\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1404 - acc: 0.9570 - val_loss: 0.1794 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1702 - acc: 0.9424 - val_loss: 0.1569 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1401 - acc: 0.9590 - val_loss: 0.4946 - val_acc: 0.9016\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1629 - acc: 0.9463 - val_loss: 0.1721 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.1539 - acc: 0.9492 - val_loss: 0.1989 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1365 - acc: 0.9561 - val_loss: 0.5344 - val_acc: 0.8196\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1546 - acc: 0.9443 - val_loss: 0.1859 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1273 - acc: 0.9551 - val_loss: 0.2152 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1520 - acc: 0.9443 - val_loss: 0.1687 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1621 - acc: 0.9434 - val_loss: 0.1484 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.15344 to 0.14836, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 39/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1560 - acc: 0.9463 - val_loss: 0.1636 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1404 - acc: 0.9531 - val_loss: 0.1544 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1661 - acc: 0.9463 - val_loss: 0.3497 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1619 - acc: 0.9453 - val_loss: 0.1650 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1453 - acc: 0.9512 - val_loss: 0.1389 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.14836 to 0.13893, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 44/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1498 - acc: 0.9512 - val_loss: 0.2834 - val_acc: 0.9242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1811 - acc: 0.9450 - val_loss: 0.1533 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1405 - acc: 0.9492 - val_loss: 0.1445 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1423 - acc: 0.9590 - val_loss: 0.1877 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1428 - acc: 0.9541 - val_loss: 0.1272 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.13893 to 0.12725, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 49/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1524 - acc: 0.9482 - val_loss: 0.1941 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1423 - acc: 0.9521 - val_loss: 0.1645 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1489 - acc: 0.9492 - val_loss: 0.1516 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1306 - acc: 0.9580 - val_loss: 0.1799 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1304 - acc: 0.9590 - val_loss: 0.1971 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1274 - acc: 0.9551 - val_loss: 0.1814 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1540 - acc: 0.9499 - val_loss: 0.1441 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1588 - acc: 0.9463 - val_loss: 0.1771 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1304 - acc: 0.9512 - val_loss: 0.1689 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1119 - acc: 0.9580 - val_loss: 0.1592 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1560 - acc: 0.9395 - val_loss: 0.1632 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1548 - acc: 0.9463 - val_loss: 0.1271 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.12725 to 0.12714, saving model to weights.best.DenseNet121-nih-one-v-all-fibrosis.20180305-r1.hdf5\n",
      "Epoch 61/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1240 - acc: 0.9609 - val_loss: 0.1692 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1242 - acc: 0.9570 - val_loss: 0.1707 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1361 - acc: 0.9521 - val_loss: 0.1647 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1234 - acc: 0.9580 - val_loss: 0.1854 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1058 - acc: 0.9668 - val_loss: 0.1514 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1338 - acc: 0.9541 - val_loss: 0.1398 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1458 - acc: 0.9502 - val_loss: 0.1648 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1325 - acc: 0.9570 - val_loss: 0.1342 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1059 - acc: 0.9688 - val_loss: 0.1934 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1054 - acc: 0.9629 - val_loss: 0.1656 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1497 - acc: 0.9473 - val_loss: 0.6209 - val_acc: 0.8996\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1418 - acc: 0.9531 - val_loss: 0.3469 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1196 - acc: 0.9619 - val_loss: 0.1710 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1402 - acc: 0.9473 - val_loss: 0.1526 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1682 - acc: 0.9385 - val_loss: 0.1480 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1332 - acc: 0.9590 - val_loss: 0.1376 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.1216 - acc: 0.9580 - val_loss: 0.1362 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1227 - acc: 0.9629 - val_loss: 0.1685 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1516 - acc: 0.9512 - val_loss: 0.1718 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1230 - acc: 0.9590 - val_loss: 0.1822 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1003 - acc: 0.9629 - val_loss: 0.1735 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1559 - acc: 0.9492 - val_loss: 0.1719 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1152 - acc: 0.9637 - val_loss: 0.1455 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0966 - acc: 0.9639 - val_loss: 0.1675 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1597 - acc: 0.9502 - val_loss: 0.1739 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1372 - acc: 0.9512 - val_loss: 0.1492 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1213 - acc: 0.9668 - val_loss: 0.1635 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.1171 - acc: 0.9561 - val_loss: 0.1444 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1084 - acc: 0.9658 - val_loss: 0.1368 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1301 - acc: 0.9512 - val_loss: 0.1416 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1046 - acc: 0.9658 - val_loss: 0.1714 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1250 - acc: 0.9600 - val_loss: 0.1350 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1537 - acc: 0.9453 - val_loss: 0.1448 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0977 - acc: 0.9658 - val_loss: 0.2174 - val_acc: 0.9129\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0990 - acc: 0.9678 - val_loss: 0.1668 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1263 - acc: 0.9609 - val_loss: 0.1661 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0907 - acc: 0.9717 - val_loss: 0.1571 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0999 - acc: 0.9619 - val_loss: 0.1473 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1127 - acc: 0.9639 - val_loss: 0.2464 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1159 - acc: 0.9629 - val_loss: 0.1557 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 101/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1243 - acc: 0.9619 - val_loss: 0.1588 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 102/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1255 - acc: 0.9600 - val_loss: 0.1618 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00102: val_loss did not improve\n",
      "Epoch 103/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1078 - acc: 0.9678 - val_loss: 0.3983 - val_acc: 0.8217\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      "Epoch 104/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1195 - acc: 0.9648 - val_loss: 0.1462 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1581 - acc: 0.9482 - val_loss: 0.1290 - val_acc: 0.9682\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0931 - acc: 0.9678 - val_loss: 0.1361 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 107/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1147 - acc: 0.9648 - val_loss: 0.1607 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00107: val_loss did not improve\n",
      "Epoch 108/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1150 - acc: 0.9639 - val_loss: 0.1510 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 109/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1470 - acc: 0.9482 - val_loss: 0.1339 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 110/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.1007 - acc: 0.9648 - val_loss: 0.1885 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 111/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0992 - acc: 0.9704 - val_loss: 0.1812 - val_acc: 0.9637\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 112/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0886 - acc: 0.9707 - val_loss: 0.1998 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 113/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1013 - acc: 0.9697 - val_loss: 0.1379 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      "Epoch 114/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0987 - acc: 0.9658 - val_loss: 0.1720 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00114: val_loss did not improve\n",
      "Epoch 115/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1184 - acc: 0.9609 - val_loss: 0.1319 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00115: val_loss did not improve\n",
      "Epoch 116/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1178 - acc: 0.9580 - val_loss: 0.1355 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 117/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1259 - acc: 0.9512 - val_loss: 0.1792 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 118/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0905 - acc: 0.9736 - val_loss: 0.1643 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00118: val_loss did not improve\n",
      "Epoch 119/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1014 - acc: 0.9695 - val_loss: 0.1730 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00119: val_loss did not improve\n",
      "Epoch 120/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1080 - acc: 0.9648 - val_loss: 0.1835 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 121/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.1004 - acc: 0.9619 - val_loss: 0.1459 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00121: val_loss did not improve\n",
      "Epoch 122/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1131 - acc: 0.9697 - val_loss: 0.1485 - val_acc: 0.9597\n",
      "\n",
      "Epoch 00122: val_loss did not improve\n",
      "Epoch 123/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1083 - acc: 0.9678 - val_loss: 0.1478 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00123: val_loss did not improve\n",
      "Epoch 124/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1331 - acc: 0.9570 - val_loss: 0.1407 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 125/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0941 - acc: 0.9717 - val_loss: 0.1349 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00125: val_loss did not improve\n",
      "Epoch 126/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0926 - acc: 0.9697 - val_loss: 0.1826 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 127/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0994 - acc: 0.9648 - val_loss: 0.1451 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 128/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1237 - acc: 0.9561 - val_loss: 0.1411 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      "Epoch 129/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0625 - acc: 0.9805 - val_loss: 0.1964 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 130/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1260 - acc: 0.9600 - val_loss: 0.1700 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 131/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1058 - acc: 0.9697 - val_loss: 0.1334 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 132/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0810 - acc: 0.9746 - val_loss: 0.1618 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 133/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0981 - acc: 0.9658 - val_loss: 0.1695 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 134/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1205 - acc: 0.9626 - val_loss: 0.1647 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      "Epoch 135/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1221 - acc: 0.9561 - val_loss: 0.3514 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00135: val_loss did not improve\n",
      "Epoch 136/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0991 - acc: 0.9678 - val_loss: 0.1849 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00136: val_loss did not improve\n",
      "Epoch 137/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1125 - acc: 0.9619 - val_loss: 0.1787 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00137: val_loss did not improve\n",
      "Epoch 138/1515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 50s 2s/step - loss: 0.1138 - acc: 0.9609 - val_loss: 0.1441 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00138: val_loss did not improve\n",
      "Epoch 139/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0830 - acc: 0.9697 - val_loss: 0.1967 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00139: val_loss did not improve\n",
      "Epoch 140/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0939 - acc: 0.9688 - val_loss: 0.1576 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00140: val_loss did not improve\n",
      "Epoch 141/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1099 - acc: 0.9658 - val_loss: 0.1975 - val_acc: 0.9232\n",
      "\n",
      "Epoch 00141: val_loss did not improve\n",
      "Epoch 142/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0839 - acc: 0.9707 - val_loss: 0.2015 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00142: val_loss did not improve\n",
      "Epoch 143/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0872 - acc: 0.9717 - val_loss: 0.1520 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00143: val_loss did not improve\n",
      "Epoch 144/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0865 - acc: 0.9717 - val_loss: 0.1898 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00144: val_loss did not improve\n",
      "Epoch 145/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0904 - acc: 0.9697 - val_loss: 0.2368 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00145: val_loss did not improve\n",
      "Epoch 146/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0951 - acc: 0.9639 - val_loss: 0.2035 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00146: val_loss did not improve\n",
      "Epoch 147/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0780 - acc: 0.9727 - val_loss: 0.1873 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00147: val_loss did not improve\n",
      "Epoch 148/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1046 - acc: 0.9648 - val_loss: 0.1282 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00148: val_loss did not improve\n",
      "Epoch 149/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1197 - acc: 0.9570 - val_loss: 0.1634 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00149: val_loss did not improve\n",
      "Epoch 150/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1015 - acc: 0.9639 - val_loss: 0.1525 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00150: val_loss did not improve\n",
      "Epoch 151/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1286 - acc: 0.9521 - val_loss: 0.1608 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00151: val_loss did not improve\n",
      "Epoch 152/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0967 - acc: 0.9658 - val_loss: 0.1755 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00152: val_loss did not improve\n",
      "Epoch 153/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1073 - acc: 0.9629 - val_loss: 0.1676 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00153: val_loss did not improve\n",
      "Epoch 154/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0775 - acc: 0.9785 - val_loss: 0.1593 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00154: val_loss did not improve\n",
      "Epoch 155/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1115 - acc: 0.9629 - val_loss: 0.1612 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00155: val_loss did not improve\n",
      "Epoch 156/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0977 - acc: 0.9678 - val_loss: 0.1439 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00156: val_loss did not improve\n",
      "Epoch 157/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1001 - acc: 0.9697 - val_loss: 0.1854 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00157: val_loss did not improve\n",
      "Epoch 158/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1121 - acc: 0.9570 - val_loss: 0.1502 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00158: val_loss did not improve\n",
      "Epoch 159/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0645 - acc: 0.9823 - val_loss: 0.1669 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00159: val_loss did not improve\n",
      "Epoch 160/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0883 - acc: 0.9678 - val_loss: 0.1810 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00160: val_loss did not improve\n",
      "Epoch 161/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0636 - acc: 0.9814 - val_loss: 0.2309 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00161: val_loss did not improve\n",
      "Epoch 162/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0757 - acc: 0.9727 - val_loss: 0.2431 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00162: val_loss did not improve\n",
      "Epoch 163/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0844 - acc: 0.9756 - val_loss: 0.1650 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00163: val_loss did not improve\n",
      "Epoch 164/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0702 - acc: 0.9785 - val_loss: 0.1868 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00164: val_loss did not improve\n",
      "Epoch 165/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0847 - acc: 0.9697 - val_loss: 0.1601 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00165: val_loss did not improve\n",
      "Epoch 166/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0977 - acc: 0.9707 - val_loss: 0.1630 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00166: val_loss did not improve\n",
      "Epoch 167/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0845 - acc: 0.9697 - val_loss: 0.1942 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00167: val_loss did not improve\n",
      "Epoch 168/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0859 - acc: 0.9678 - val_loss: 0.1946 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00168: val_loss did not improve\n",
      "Epoch 169/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0976 - acc: 0.9727 - val_loss: 0.1453 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00169: val_loss did not improve\n",
      "Epoch 170/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1181 - acc: 0.9541 - val_loss: 0.1396 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00170: val_loss did not improve\n",
      "Epoch 171/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0819 - acc: 0.9717 - val_loss: 0.1874 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00171: val_loss did not improve\n",
      "Epoch 172/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1167 - acc: 0.9531 - val_loss: 0.1903 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00172: val_loss did not improve\n",
      "Epoch 173/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0926 - acc: 0.9629 - val_loss: 0.1775 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00173: val_loss did not improve\n",
      "Epoch 174/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1080 - acc: 0.9629 - val_loss: 0.1745 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00174: val_loss did not improve\n",
      "Epoch 175/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0794 - acc: 0.9725 - val_loss: 0.1568 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00175: val_loss did not improve\n",
      "Epoch 176/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0996 - acc: 0.9668 - val_loss: 0.1631 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00176: val_loss did not improve\n",
      "Epoch 177/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0932 - acc: 0.9688 - val_loss: 0.1604 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00177: val_loss did not improve\n",
      "Epoch 178/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0677 - acc: 0.9795 - val_loss: 0.1954 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00178: val_loss did not improve\n",
      "Epoch 179/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0831 - acc: 0.9688 - val_loss: 0.1734 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00179: val_loss did not improve\n",
      "Epoch 180/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0677 - acc: 0.9766 - val_loss: 0.1634 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00180: val_loss did not improve\n",
      "Epoch 181/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0767 - acc: 0.9717 - val_loss: 0.1896 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00181: val_loss did not improve\n",
      "Epoch 182/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0887 - acc: 0.9727 - val_loss: 0.1341 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00182: val_loss did not improve\n",
      "Epoch 183/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1107 - acc: 0.9619 - val_loss: 0.1654 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00183: val_loss did not improve\n",
      "Epoch 184/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0707 - acc: 0.9746 - val_loss: 0.2510 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00184: val_loss did not improve\n",
      "Epoch 185/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0808 - acc: 0.9688 - val_loss: 0.1864 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00185: val_loss did not improve\n",
      "Epoch 186/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0697 - acc: 0.9766 - val_loss: 0.1818 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00186: val_loss did not improve\n",
      "Epoch 187/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0936 - acc: 0.9688 - val_loss: 0.1973 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00187: val_loss did not improve\n",
      "Epoch 188/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0586 - acc: 0.9844 - val_loss: 0.2046 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00188: val_loss did not improve\n",
      "Epoch 189/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0732 - acc: 0.9736 - val_loss: 0.2199 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00189: val_loss did not improve\n",
      "Epoch 190/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0915 - acc: 0.9658 - val_loss: 0.1678 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00190: val_loss did not improve\n",
      "Epoch 191/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0622 - acc: 0.9834 - val_loss: 0.1747 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00191: val_loss did not improve\n",
      "Epoch 192/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.1005 - acc: 0.9639 - val_loss: 0.2174 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00192: val_loss did not improve\n",
      "Epoch 193/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0635 - acc: 0.9795 - val_loss: 0.2253 - val_acc: 0.9273\n",
      "\n",
      "Epoch 00193: val_loss did not improve\n",
      "Epoch 194/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0827 - acc: 0.9717 - val_loss: 0.1807 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00194: val_loss did not improve\n",
      "Epoch 195/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0902 - acc: 0.9746 - val_loss: 0.1889 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00195: val_loss did not improve\n",
      "Epoch 196/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0678 - acc: 0.9814 - val_loss: 0.1764 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00196: val_loss did not improve\n",
      "Epoch 197/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0560 - acc: 0.9814 - val_loss: 0.2054 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00197: val_loss did not improve\n",
      "Epoch 198/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0931 - acc: 0.9626 - val_loss: 0.2762 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00198: val_loss did not improve\n",
      "Epoch 199/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0962 - acc: 0.9736 - val_loss: 0.1689 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00199: val_loss did not improve\n",
      "Epoch 200/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0844 - acc: 0.9707 - val_loss: 0.1543 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00200: val_loss did not improve\n",
      "Epoch 201/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0685 - acc: 0.9746 - val_loss: 0.1535 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00201: val_loss did not improve\n",
      "Epoch 202/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0670 - acc: 0.9795 - val_loss: 0.2158 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00202: val_loss did not improve\n",
      "Epoch 203/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0876 - acc: 0.9697 - val_loss: 0.2249 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00203: val_loss did not improve\n",
      "Epoch 204/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0738 - acc: 0.9775 - val_loss: 0.2037 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00204: val_loss did not improve\n",
      "Epoch 205/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0872 - acc: 0.9707 - val_loss: 0.2036 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00205: val_loss did not improve\n",
      "Epoch 206/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0565 - acc: 0.9834 - val_loss: 0.2531 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00206: val_loss did not improve\n",
      "Epoch 207/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0769 - acc: 0.9697 - val_loss: 0.1646 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00207: val_loss did not improve\n",
      "Epoch 208/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0583 - acc: 0.9805 - val_loss: 0.1578 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00208: val_loss did not improve\n",
      "Epoch 209/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0867 - acc: 0.9717 - val_loss: 0.1917 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00209: val_loss did not improve\n",
      "Epoch 210/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0529 - acc: 0.9805 - val_loss: 0.2521 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00210: val_loss did not improve\n",
      "Epoch 211/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0840 - acc: 0.9756 - val_loss: 0.1698 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00211: val_loss did not improve\n",
      "Epoch 212/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0669 - acc: 0.9814 - val_loss: 0.2654 - val_acc: 0.9232\n",
      "\n",
      "Epoch 00212: val_loss did not improve\n",
      "Epoch 213/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0723 - acc: 0.9746 - val_loss: 0.1573 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00213: val_loss did not improve\n",
      "Epoch 214/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0831 - acc: 0.9725 - val_loss: 0.1596 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00214: val_loss did not improve\n",
      "Epoch 215/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0874 - acc: 0.9746 - val_loss: 0.1584 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00215: val_loss did not improve\n",
      "Epoch 216/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0833 - acc: 0.9727 - val_loss: 0.2336 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00216: val_loss did not improve\n",
      "Epoch 217/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0743 - acc: 0.9795 - val_loss: 0.1953 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00217: val_loss did not improve\n",
      "Epoch 218/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0718 - acc: 0.9736 - val_loss: 0.1569 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00218: val_loss did not improve\n",
      "Epoch 219/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0767 - acc: 0.9697 - val_loss: 0.1936 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00219: val_loss did not improve\n",
      "Epoch 220/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0902 - acc: 0.9717 - val_loss: 0.1971 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00220: val_loss did not improve\n",
      "Epoch 221/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0688 - acc: 0.9785 - val_loss: 0.1778 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00221: val_loss did not improve\n",
      "Epoch 222/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0585 - acc: 0.9814 - val_loss: 0.1891 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00222: val_loss did not improve\n",
      "Epoch 223/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0530 - acc: 0.9795 - val_loss: 0.2188 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00223: val_loss did not improve\n",
      "Epoch 224/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0456 - acc: 0.9844 - val_loss: 0.2294 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00224: val_loss did not improve\n",
      "Epoch 225/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0606 - acc: 0.9844 - val_loss: 0.1900 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00225: val_loss did not improve\n",
      "Epoch 226/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0756 - acc: 0.9766 - val_loss: 0.2540 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00226: val_loss did not improve\n",
      "Epoch 227/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0793 - acc: 0.9756 - val_loss: 0.2526 - val_acc: 0.9027\n",
      "\n",
      "Epoch 00227: val_loss did not improve\n",
      "Epoch 228/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0672 - acc: 0.9766 - val_loss: 0.3724 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00228: val_loss did not improve\n",
      "Epoch 229/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0857 - acc: 0.9658 - val_loss: 0.1937 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00229: val_loss did not improve\n",
      "Epoch 230/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0559 - acc: 0.9805 - val_loss: 0.2369 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00230: val_loss did not improve\n",
      "Epoch 231/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0571 - acc: 0.9844 - val_loss: 0.2099 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00231: val_loss did not improve\n",
      "Epoch 232/1515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 49s 2s/step - loss: 0.0899 - acc: 0.9725 - val_loss: 0.3758 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00232: val_loss did not improve\n",
      "Epoch 233/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0797 - acc: 0.9707 - val_loss: 0.1729 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00233: val_loss did not improve\n",
      "Epoch 234/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0582 - acc: 0.9814 - val_loss: 0.2056 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00234: val_loss did not improve\n",
      "Epoch 235/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0635 - acc: 0.9766 - val_loss: 0.2331 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00235: val_loss did not improve\n",
      "Epoch 236/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0780 - acc: 0.9727 - val_loss: 0.2146 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00236: val_loss did not improve\n",
      "Epoch 237/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0815 - acc: 0.9717 - val_loss: 0.1647 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00237: val_loss did not improve\n",
      "Epoch 238/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0681 - acc: 0.9785 - val_loss: 0.2013 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00238: val_loss did not improve\n",
      "Epoch 239/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0446 - acc: 0.9824 - val_loss: 0.1573 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00239: val_loss did not improve\n",
      "Epoch 240/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0576 - acc: 0.9775 - val_loss: 0.2275 - val_acc: 0.9221\n",
      "\n",
      "Epoch 00240: val_loss did not improve\n",
      "Epoch 241/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0612 - acc: 0.9795 - val_loss: 0.2025 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00241: val_loss did not improve\n",
      "Epoch 242/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0612 - acc: 0.9727 - val_loss: 0.3004 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00242: val_loss did not improve\n",
      "Epoch 243/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0656 - acc: 0.9775 - val_loss: 0.2112 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00243: val_loss did not improve\n",
      "Epoch 244/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0569 - acc: 0.9785 - val_loss: 0.2184 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00244: val_loss did not improve\n",
      "Epoch 245/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0538 - acc: 0.9766 - val_loss: 0.2839 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00245: val_loss did not improve\n",
      "Epoch 246/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0945 - acc: 0.9639 - val_loss: 0.3073 - val_acc: 0.8893\n",
      "\n",
      "Epoch 00246: val_loss did not improve\n",
      "Epoch 247/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0640 - acc: 0.9756 - val_loss: 0.1774 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00247: val_loss did not improve\n",
      "Epoch 248/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0520 - acc: 0.9785 - val_loss: 0.2450 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00248: val_loss did not improve\n",
      "Epoch 249/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0587 - acc: 0.9746 - val_loss: 0.1801 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00249: val_loss did not improve\n",
      "Epoch 250/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0744 - acc: 0.9727 - val_loss: 0.1896 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00250: val_loss did not improve\n",
      "Epoch 251/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0738 - acc: 0.9754 - val_loss: 0.1842 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00251: val_loss did not improve\n",
      "Epoch 252/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0619 - acc: 0.9756 - val_loss: 0.1913 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00252: val_loss did not improve\n",
      "Epoch 253/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0892 - acc: 0.9756 - val_loss: 0.1725 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00253: val_loss did not improve\n",
      "Epoch 254/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0380 - acc: 0.9863 - val_loss: 0.2141 - val_acc: 0.9486\n",
      "\n",
      "Epoch 00254: val_loss did not improve\n",
      "Epoch 255/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0462 - acc: 0.9805 - val_loss: 0.1927 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00255: val_loss did not improve\n",
      "Epoch 256/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0618 - acc: 0.9775 - val_loss: 0.2551 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00256: val_loss did not improve\n",
      "Epoch 257/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0597 - acc: 0.9785 - val_loss: 0.1895 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00257: val_loss did not improve\n",
      "Epoch 258/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0547 - acc: 0.9824 - val_loss: 0.1909 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00258: val_loss did not improve\n",
      "Epoch 259/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0683 - acc: 0.9756 - val_loss: 0.2365 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00259: val_loss did not improve\n",
      "Epoch 260/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0673 - acc: 0.9785 - val_loss: 0.1990 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00260: val_loss did not improve\n",
      "Epoch 261/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0527 - acc: 0.9863 - val_loss: 0.1806 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00261: val_loss did not improve\n",
      "Epoch 262/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0706 - acc: 0.9756 - val_loss: 0.2144 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00262: val_loss did not improve\n",
      "Epoch 263/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0390 - acc: 0.9902 - val_loss: 0.2315 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00263: val_loss did not improve\n",
      "Epoch 264/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0565 - acc: 0.9766 - val_loss: 0.1683 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00264: val_loss did not improve\n",
      "Epoch 265/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0632 - acc: 0.9805 - val_loss: 0.2076 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00265: val_loss did not improve\n",
      "Epoch 266/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0641 - acc: 0.9756 - val_loss: 0.1866 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00266: val_loss did not improve\n",
      "Epoch 267/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0658 - acc: 0.9775 - val_loss: 0.2422 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00267: val_loss did not improve\n",
      "Epoch 268/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0510 - acc: 0.9834 - val_loss: 0.2193 - val_acc: 0.9273\n",
      "\n",
      "Epoch 00268: val_loss did not improve\n",
      "Epoch 269/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0505 - acc: 0.9756 - val_loss: 0.3217 - val_acc: 0.9119\n",
      "\n",
      "Epoch 00269: val_loss did not improve\n",
      "Epoch 270/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0469 - acc: 0.9824 - val_loss: 0.2459 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00270: val_loss did not improve\n",
      "Epoch 271/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0594 - acc: 0.9766 - val_loss: 0.1864 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00271: val_loss did not improve\n",
      "Epoch 272/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0696 - acc: 0.9746 - val_loss: 0.2289 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00272: val_loss did not improve\n",
      "Epoch 273/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0422 - acc: 0.9854 - val_loss: 0.1964 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00273: val_loss did not improve\n",
      "Epoch 274/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0386 - acc: 0.9824 - val_loss: 0.2040 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00274: val_loss did not improve\n",
      "Epoch 275/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0454 - acc: 0.9834 - val_loss: 0.1455 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00275: val_loss did not improve\n",
      "Epoch 276/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0561 - acc: 0.9795 - val_loss: 0.1726 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00276: val_loss did not improve\n",
      "Epoch 277/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0453 - acc: 0.9863 - val_loss: 0.1999 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00277: val_loss did not improve\n",
      "Epoch 278/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0681 - acc: 0.9756 - val_loss: 0.2141 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00278: val_loss did not improve\n",
      "Epoch 279/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0632 - acc: 0.9766 - val_loss: 0.1599 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00279: val_loss did not improve\n",
      "Epoch 280/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0822 - acc: 0.9688 - val_loss: 0.1926 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00280: val_loss did not improve\n",
      "Epoch 281/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0624 - acc: 0.9814 - val_loss: 0.2157 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00281: val_loss did not improve\n",
      "Epoch 282/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0566 - acc: 0.9805 - val_loss: 0.3021 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00282: val_loss did not improve\n",
      "Epoch 283/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.1106 - acc: 0.9521 - val_loss: 0.1936 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00283: val_loss did not improve\n",
      "Epoch 284/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0571 - acc: 0.9795 - val_loss: 0.2186 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00284: val_loss did not improve\n",
      "Epoch 285/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0463 - acc: 0.9854 - val_loss: 0.3060 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00285: val_loss did not improve\n",
      "Epoch 286/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0373 - acc: 0.9883 - val_loss: 0.2078 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00286: val_loss did not improve\n",
      "Epoch 287/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0301 - acc: 0.9893 - val_loss: 0.2453 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00287: val_loss did not improve\n",
      "Epoch 288/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0507 - acc: 0.9834 - val_loss: 0.3375 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00288: val_loss did not improve\n",
      "Epoch 289/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0296 - acc: 0.9902 - val_loss: 0.2335 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00289: val_loss did not improve\n",
      "Epoch 290/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0725 - acc: 0.9756 - val_loss: 0.2605 - val_acc: 0.9129\n",
      "\n",
      "Epoch 00290: val_loss did not improve\n",
      "Epoch 291/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0385 - acc: 0.9873 - val_loss: 0.2456 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00291: val_loss did not improve\n",
      "Epoch 292/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0554 - acc: 0.9775 - val_loss: 0.1922 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00292: val_loss did not improve\n",
      "Epoch 293/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0513 - acc: 0.9795 - val_loss: 0.2337 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00293: val_loss did not improve\n",
      "Epoch 294/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0514 - acc: 0.9834 - val_loss: 0.2100 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00294: val_loss did not improve\n",
      "Epoch 295/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0498 - acc: 0.9824 - val_loss: 0.2938 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00295: val_loss did not improve\n",
      "Epoch 296/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0547 - acc: 0.9814 - val_loss: 0.2424 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00296: val_loss did not improve\n",
      "Epoch 297/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0377 - acc: 0.9844 - val_loss: 0.2609 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00297: val_loss did not improve\n",
      "Epoch 298/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0444 - acc: 0.9814 - val_loss: 0.2747 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00298: val_loss did not improve\n",
      "Epoch 299/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0688 - acc: 0.9746 - val_loss: 0.1939 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00299: val_loss did not improve\n",
      "Epoch 300/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0481 - acc: 0.9805 - val_loss: 0.1848 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00300: val_loss did not improve\n",
      "Epoch 301/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0260 - acc: 0.9902 - val_loss: 0.2628 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00301: val_loss did not improve\n",
      "Epoch 302/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0201 - acc: 0.9922 - val_loss: 0.2920 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00302: val_loss did not improve\n",
      "Epoch 303/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0315 - acc: 0.9863 - val_loss: 0.3078 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00303: val_loss did not improve\n",
      "Epoch 304/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0471 - acc: 0.9824 - val_loss: 0.3337 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00304: val_loss did not improve\n",
      "Epoch 305/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0689 - acc: 0.9717 - val_loss: 0.3011 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00305: val_loss did not improve\n",
      "Epoch 306/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0437 - acc: 0.9844 - val_loss: 0.2466 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00306: val_loss did not improve\n",
      "Epoch 307/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0545 - acc: 0.9795 - val_loss: 0.3188 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00307: val_loss did not improve\n",
      "Epoch 308/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0552 - acc: 0.9805 - val_loss: 0.2271 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00308: val_loss did not improve\n",
      "Epoch 309/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0402 - acc: 0.9834 - val_loss: 0.2332 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00309: val_loss did not improve\n",
      "Epoch 310/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0277 - acc: 0.9902 - val_loss: 0.2590 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00310: val_loss did not improve\n",
      "Epoch 311/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0396 - acc: 0.9863 - val_loss: 0.2679 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00311: val_loss did not improve\n",
      "Epoch 312/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0474 - acc: 0.9824 - val_loss: 0.2675 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00312: val_loss did not improve\n",
      "Epoch 313/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0476 - acc: 0.9854 - val_loss: 0.2942 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00313: val_loss did not improve\n",
      "Epoch 314/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0644 - acc: 0.9775 - val_loss: 0.2311 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00314: val_loss did not improve\n",
      "Epoch 315/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0342 - acc: 0.9863 - val_loss: 0.1788 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00315: val_loss did not improve\n",
      "Epoch 316/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0406 - acc: 0.9834 - val_loss: 0.2601 - val_acc: 0.9242\n",
      "\n",
      "Epoch 00316: val_loss did not improve\n",
      "Epoch 317/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0395 - acc: 0.9824 - val_loss: 0.3196 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00317: val_loss did not improve\n",
      "Epoch 318/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0332 - acc: 0.9902 - val_loss: 0.2281 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00318: val_loss did not improve\n",
      "Epoch 319/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0497 - acc: 0.9795 - val_loss: 0.2758 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00319: val_loss did not improve\n",
      "Epoch 320/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0753 - acc: 0.9727 - val_loss: 0.2992 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00320: val_loss did not improve\n",
      "Epoch 321/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0615 - acc: 0.9775 - val_loss: 0.1803 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00321: val_loss did not improve\n",
      "Epoch 322/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0352 - acc: 0.9873 - val_loss: 0.2150 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00322: val_loss did not improve\n",
      "Epoch 323/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0256 - acc: 0.9912 - val_loss: 0.1984 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00323: val_loss did not improve\n",
      "Epoch 324/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0426 - acc: 0.9854 - val_loss: 0.2276 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00324: val_loss did not improve\n",
      "Epoch 325/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0550 - acc: 0.9785 - val_loss: 0.2573 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00325: val_loss did not improve\n",
      "Epoch 326/1515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 50s 2s/step - loss: 0.0336 - acc: 0.9893 - val_loss: 0.2385 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00326: val_loss did not improve\n",
      "Epoch 327/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0467 - acc: 0.9844 - val_loss: 0.2945 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00327: val_loss did not improve\n",
      "Epoch 328/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0411 - acc: 0.9844 - val_loss: 0.2375 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00328: val_loss did not improve\n",
      "Epoch 329/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0413 - acc: 0.9824 - val_loss: 0.2643 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00329: val_loss did not improve\n",
      "Epoch 330/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0609 - acc: 0.9775 - val_loss: 0.2187 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00330: val_loss did not improve\n",
      "Epoch 331/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0424 - acc: 0.9854 - val_loss: 0.3199 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00331: val_loss did not improve\n",
      "Epoch 332/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0472 - acc: 0.9785 - val_loss: 0.2220 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00332: val_loss did not improve\n",
      "Epoch 333/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0297 - acc: 0.9922 - val_loss: 0.2011 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00333: val_loss did not improve\n",
      "Epoch 334/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0208 - acc: 0.9932 - val_loss: 0.2834 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00334: val_loss did not improve\n",
      "Epoch 335/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0220 - acc: 0.9941 - val_loss: 0.2432 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00335: val_loss did not improve\n",
      "Epoch 336/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0271 - acc: 0.9902 - val_loss: 0.3052 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00336: val_loss did not improve\n",
      "Epoch 337/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0183 - acc: 0.9922 - val_loss: 0.2620 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00337: val_loss did not improve\n",
      "Epoch 338/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0501 - acc: 0.9824 - val_loss: 0.2534 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00338: val_loss did not improve\n",
      "Epoch 339/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0451 - acc: 0.9844 - val_loss: 0.2238 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00339: val_loss did not improve\n",
      "Epoch 340/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0381 - acc: 0.9883 - val_loss: 0.2387 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00340: val_loss did not improve\n",
      "Epoch 341/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0163 - acc: 0.9932 - val_loss: 0.2494 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00341: val_loss did not improve\n",
      "Epoch 342/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0443 - acc: 0.9844 - val_loss: 0.3007 - val_acc: 0.9526\n",
      "\n",
      "Epoch 00342: val_loss did not improve\n",
      "Epoch 343/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0578 - acc: 0.9766 - val_loss: 0.2592 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00343: val_loss did not improve\n",
      "Epoch 344/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0435 - acc: 0.9863 - val_loss: 0.2561 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00344: val_loss did not improve\n",
      "Epoch 345/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0500 - acc: 0.9795 - val_loss: 0.2998 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00345: val_loss did not improve\n",
      "Epoch 346/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0350 - acc: 0.9873 - val_loss: 0.2812 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00346: val_loss did not improve\n",
      "Epoch 347/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0507 - acc: 0.9834 - val_loss: 0.2593 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00347: val_loss did not improve\n",
      "Epoch 348/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0484 - acc: 0.9854 - val_loss: 0.2663 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00348: val_loss did not improve\n",
      "Epoch 349/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0223 - acc: 0.9893 - val_loss: 0.3095 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00349: val_loss did not improve\n",
      "Epoch 350/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0196 - acc: 0.9932 - val_loss: 0.3041 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00350: val_loss did not improve\n",
      "Epoch 351/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0261 - acc: 0.9902 - val_loss: 0.2972 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00351: val_loss did not improve\n",
      "Epoch 352/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0240 - acc: 0.9941 - val_loss: 0.3153 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00352: val_loss did not improve\n",
      "Epoch 353/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0274 - acc: 0.9922 - val_loss: 0.3184 - val_acc: 0.9466\n",
      "\n",
      "Epoch 00353: val_loss did not improve\n",
      "Epoch 354/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0316 - acc: 0.9863 - val_loss: 0.3021 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00354: val_loss did not improve\n",
      "Epoch 355/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0494 - acc: 0.9834 - val_loss: 0.2703 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00355: val_loss did not improve\n",
      "Epoch 356/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0446 - acc: 0.9854 - val_loss: 0.2517 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00356: val_loss did not improve\n",
      "Epoch 357/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0327 - acc: 0.9883 - val_loss: 0.2698 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00357: val_loss did not improve\n",
      "Epoch 358/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0398 - acc: 0.9863 - val_loss: 0.2154 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00358: val_loss did not improve\n",
      "Epoch 359/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0245 - acc: 0.9912 - val_loss: 0.2445 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00359: val_loss did not improve\n",
      "Epoch 360/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0284 - acc: 0.9912 - val_loss: 0.2602 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00360: val_loss did not improve\n",
      "Epoch 361/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0193 - acc: 0.9941 - val_loss: 0.3515 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00361: val_loss did not improve\n",
      "Epoch 362/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0285 - acc: 0.9893 - val_loss: 0.2926 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00362: val_loss did not improve\n",
      "Epoch 363/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0314 - acc: 0.9854 - val_loss: 0.4082 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00363: val_loss did not improve\n",
      "Epoch 364/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0331 - acc: 0.9893 - val_loss: 0.2907 - val_acc: 0.9345\n",
      "\n",
      "Epoch 00364: val_loss did not improve\n",
      "Epoch 365/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0324 - acc: 0.9854 - val_loss: 0.3054 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00365: val_loss did not improve\n",
      "Epoch 366/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0304 - acc: 0.9922 - val_loss: 0.3221 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00366: val_loss did not improve\n",
      "Epoch 367/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0357 - acc: 0.9863 - val_loss: 0.2244 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00367: val_loss did not improve\n",
      "Epoch 368/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0298 - acc: 0.9873 - val_loss: 0.4456 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00368: val_loss did not improve\n",
      "Epoch 369/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0623 - acc: 0.9756 - val_loss: 0.2297 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00369: val_loss did not improve\n",
      "Epoch 370/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0381 - acc: 0.9854 - val_loss: 0.3802 - val_acc: 0.9150\n",
      "\n",
      "Epoch 00370: val_loss did not improve\n",
      "Epoch 371/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0442 - acc: 0.9834 - val_loss: 0.2411 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00371: val_loss did not improve\n",
      "Epoch 372/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0302 - acc: 0.9883 - val_loss: 0.3251 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00372: val_loss did not improve\n",
      "Epoch 373/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0223 - acc: 0.9912 - val_loss: 0.3353 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00373: val_loss did not improve\n",
      "Epoch 374/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0461 - acc: 0.9844 - val_loss: 0.3249 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00374: val_loss did not improve\n",
      "Epoch 375/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0345 - acc: 0.9873 - val_loss: 0.2229 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00375: val_loss did not improve\n",
      "Epoch 376/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0436 - acc: 0.9873 - val_loss: 0.2569 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00376: val_loss did not improve\n",
      "Epoch 377/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0359 - acc: 0.9932 - val_loss: 0.2389 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00377: val_loss did not improve\n",
      "Epoch 378/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0202 - acc: 0.9932 - val_loss: 0.2538 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00378: val_loss did not improve\n",
      "Epoch 379/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0268 - acc: 0.9873 - val_loss: 0.2925 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00379: val_loss did not improve\n",
      "Epoch 380/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0130 - acc: 0.9951 - val_loss: 0.3564 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00380: val_loss did not improve\n",
      "Epoch 381/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0178 - acc: 0.9941 - val_loss: 0.2758 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00381: val_loss did not improve\n",
      "Epoch 382/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0112 - acc: 0.9971 - val_loss: 0.2685 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00382: val_loss did not improve\n",
      "Epoch 383/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0135 - acc: 0.9951 - val_loss: 0.2601 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00383: val_loss did not improve\n",
      "Epoch 384/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0163 - acc: 0.9922 - val_loss: 0.3774 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00384: val_loss did not improve\n",
      "Epoch 385/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0191 - acc: 0.9912 - val_loss: 0.3398 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00385: val_loss did not improve\n",
      "Epoch 386/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0451 - acc: 0.9873 - val_loss: 0.2784 - val_acc: 0.9486\n",
      "\n",
      "Epoch 00386: val_loss did not improve\n",
      "Epoch 387/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0194 - acc: 0.9941 - val_loss: 0.2934 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00387: val_loss did not improve\n",
      "Epoch 388/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0197 - acc: 0.9922 - val_loss: 0.3503 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00388: val_loss did not improve\n",
      "Epoch 389/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0363 - acc: 0.9902 - val_loss: 0.3385 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00389: val_loss did not improve\n",
      "Epoch 390/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0261 - acc: 0.9902 - val_loss: 0.3853 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00390: val_loss did not improve\n",
      "Epoch 391/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0245 - acc: 0.9902 - val_loss: 0.4280 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00391: val_loss did not improve\n",
      "Epoch 392/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0327 - acc: 0.9863 - val_loss: 0.2886 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00392: val_loss did not improve\n",
      "Epoch 393/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0294 - acc: 0.9902 - val_loss: 0.2603 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00393: val_loss did not improve\n",
      "Epoch 394/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0369 - acc: 0.9863 - val_loss: 0.3787 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00394: val_loss did not improve\n",
      "Epoch 395/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0775 - acc: 0.9688 - val_loss: 0.2880 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00395: val_loss did not improve\n",
      "Epoch 396/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0275 - acc: 0.9854 - val_loss: 0.2774 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00396: val_loss did not improve\n",
      "Epoch 397/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0259 - acc: 0.9922 - val_loss: 0.2673 - val_acc: 0.9415\n",
      "\n",
      "Epoch 00397: val_loss did not improve\n",
      "Epoch 398/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0098 - acc: 0.9971 - val_loss: 0.3057 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00398: val_loss did not improve\n",
      "Epoch 399/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0246 - acc: 0.9922 - val_loss: 0.2638 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00399: val_loss did not improve\n",
      "Epoch 400/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0359 - acc: 0.9863 - val_loss: 0.2800 - val_acc: 0.9273\n",
      "\n",
      "Epoch 00400: val_loss did not improve\n",
      "Epoch 401/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0258 - acc: 0.9922 - val_loss: 0.2571 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00401: val_loss did not improve\n",
      "Epoch 402/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0210 - acc: 0.9932 - val_loss: 0.2880 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00402: val_loss did not improve\n",
      "Epoch 403/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0338 - acc: 0.9854 - val_loss: 0.2909 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00403: val_loss did not improve\n",
      "Epoch 404/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0480 - acc: 0.9844 - val_loss: 0.2641 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00404: val_loss did not improve\n",
      "Epoch 405/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0463 - acc: 0.9844 - val_loss: 0.2498 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00405: val_loss did not improve\n",
      "Epoch 406/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0225 - acc: 0.9922 - val_loss: 0.2758 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00406: val_loss did not improve\n",
      "Epoch 407/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0200 - acc: 0.9971 - val_loss: 0.3101 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00407: val_loss did not improve\n",
      "Epoch 408/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0137 - acc: 0.9961 - val_loss: 0.3103 - val_acc: 0.9466\n",
      "\n",
      "Epoch 00408: val_loss did not improve\n",
      "Epoch 409/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0231 - acc: 0.9932 - val_loss: 0.3339 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00409: val_loss did not improve\n",
      "Epoch 410/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0463 - acc: 0.9854 - val_loss: 0.2471 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00410: val_loss did not improve\n",
      "Epoch 411/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0277 - acc: 0.9922 - val_loss: 0.2232 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00411: val_loss did not improve\n",
      "Epoch 412/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0176 - acc: 0.9922 - val_loss: 0.3418 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00412: val_loss did not improve\n",
      "Epoch 413/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0354 - acc: 0.9893 - val_loss: 0.3247 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00413: val_loss did not improve\n",
      "Epoch 414/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0249 - acc: 0.9902 - val_loss: 0.3013 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00414: val_loss did not improve\n",
      "Epoch 415/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0364 - acc: 0.9912 - val_loss: 0.2468 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00415: val_loss did not improve\n",
      "Epoch 416/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0185 - acc: 0.9941 - val_loss: 0.2852 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00416: val_loss did not improve\n",
      "Epoch 417/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.2898 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00417: val_loss did not improve\n",
      "Epoch 418/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0169 - acc: 0.9932 - val_loss: 0.3151 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00418: val_loss did not improve\n",
      "Epoch 419/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0204 - acc: 0.9932 - val_loss: 0.2689 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00419: val_loss did not improve\n",
      "Epoch 420/1515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 50s 2s/step - loss: 0.0368 - acc: 0.9873 - val_loss: 0.2855 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00420: val_loss did not improve\n",
      "Epoch 421/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0215 - acc: 0.9961 - val_loss: 0.2232 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00421: val_loss did not improve\n",
      "Epoch 422/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0222 - acc: 0.9941 - val_loss: 0.2648 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00422: val_loss did not improve\n",
      "Epoch 423/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0237 - acc: 0.9932 - val_loss: 0.3988 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00423: val_loss did not improve\n",
      "Epoch 424/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0221 - acc: 0.9941 - val_loss: 0.3535 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00424: val_loss did not improve\n",
      "Epoch 425/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0319 - acc: 0.9873 - val_loss: 0.3393 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00425: val_loss did not improve\n",
      "Epoch 426/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0162 - acc: 0.9932 - val_loss: 0.2705 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00426: val_loss did not improve\n",
      "Epoch 427/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0263 - acc: 0.9922 - val_loss: 0.3761 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00427: val_loss did not improve\n",
      "Epoch 428/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0211 - acc: 0.9941 - val_loss: 0.3422 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00428: val_loss did not improve\n",
      "Epoch 429/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0223 - acc: 0.9941 - val_loss: 0.3961 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00429: val_loss did not improve\n",
      "Epoch 430/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0585 - acc: 0.9795 - val_loss: 0.2527 - val_acc: 0.9526\n",
      "\n",
      "Epoch 00430: val_loss did not improve\n",
      "Epoch 431/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0175 - acc: 0.9941 - val_loss: 0.2911 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00431: val_loss did not improve\n",
      "Epoch 432/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0272 - acc: 0.9912 - val_loss: 0.3340 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00432: val_loss did not improve\n",
      "Epoch 433/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0270 - acc: 0.9902 - val_loss: 0.3952 - val_acc: 0.9283\n",
      "\n",
      "Epoch 00433: val_loss did not improve\n",
      "Epoch 434/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0278 - acc: 0.9883 - val_loss: 0.3668 - val_acc: 0.9221\n",
      "\n",
      "Epoch 00434: val_loss did not improve\n",
      "Epoch 435/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0246 - acc: 0.9912 - val_loss: 0.3234 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00435: val_loss did not improve\n",
      "Epoch 436/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0294 - acc: 0.9902 - val_loss: 0.2542 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00436: val_loss did not improve\n",
      "Epoch 437/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0129 - acc: 0.9961 - val_loss: 0.2721 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00437: val_loss did not improve\n",
      "Epoch 438/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0162 - acc: 0.9932 - val_loss: 0.3431 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00438: val_loss did not improve\n",
      "Epoch 439/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0134 - acc: 0.9961 - val_loss: 0.2824 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00439: val_loss did not improve\n",
      "Epoch 440/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0196 - acc: 0.9932 - val_loss: 0.3519 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00440: val_loss did not improve\n",
      "Epoch 441/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0428 - acc: 0.9873 - val_loss: 0.2877 - val_acc: 0.9637\n",
      "\n",
      "Epoch 00441: val_loss did not improve\n",
      "Epoch 442/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0401 - acc: 0.9854 - val_loss: 0.2079 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00442: val_loss did not improve\n",
      "Epoch 443/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0320 - acc: 0.9893 - val_loss: 0.2103 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00443: val_loss did not improve\n",
      "Epoch 444/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0106 - acc: 0.9980 - val_loss: 0.2125 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00444: val_loss did not improve\n",
      "Epoch 445/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0122 - acc: 0.9951 - val_loss: 0.3174 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00445: val_loss did not improve\n",
      "Epoch 446/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0118 - acc: 0.9951 - val_loss: 0.2814 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00446: val_loss did not improve\n",
      "Epoch 447/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0165 - acc: 0.9951 - val_loss: 0.2254 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00447: val_loss did not improve\n",
      "Epoch 448/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0127 - acc: 0.9951 - val_loss: 0.3396 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00448: val_loss did not improve\n",
      "Epoch 449/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0091 - acc: 0.9980 - val_loss: 0.3615 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00449: val_loss did not improve\n",
      "Epoch 450/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0107 - acc: 0.9980 - val_loss: 0.3794 - val_acc: 0.9242\n",
      "\n",
      "Epoch 00450: val_loss did not improve\n",
      "Epoch 451/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0102 - acc: 0.9971 - val_loss: 0.4273 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00451: val_loss did not improve\n",
      "Epoch 452/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0327 - acc: 0.9912 - val_loss: 0.2959 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00452: val_loss did not improve\n",
      "Epoch 453/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0196 - acc: 0.9932 - val_loss: 0.3560 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00453: val_loss did not improve\n",
      "Epoch 454/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0430 - acc: 0.9862 - val_loss: 0.3124 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00454: val_loss did not improve\n",
      "Epoch 455/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0214 - acc: 0.9922 - val_loss: 0.2745 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00455: val_loss did not improve\n",
      "Epoch 456/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0234 - acc: 0.9902 - val_loss: 0.3065 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00456: val_loss did not improve\n",
      "Epoch 457/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0322 - acc: 0.9883 - val_loss: 0.3049 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00457: val_loss did not improve\n",
      "Epoch 458/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0171 - acc: 0.9941 - val_loss: 0.2507 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00458: val_loss did not improve\n",
      "Epoch 459/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0105 - acc: 0.9980 - val_loss: 0.2773 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00459: val_loss did not improve\n",
      "Epoch 460/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0176 - acc: 0.9941 - val_loss: 0.2979 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00460: val_loss did not improve\n",
      "Epoch 461/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0196 - acc: 0.9941 - val_loss: 0.3092 - val_acc: 0.9078\n",
      "\n",
      "Epoch 00461: val_loss did not improve\n",
      "Epoch 462/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0150 - acc: 0.9941 - val_loss: 0.3556 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00462: val_loss did not improve\n",
      "Epoch 463/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0107 - acc: 0.9951 - val_loss: 0.3205 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00463: val_loss did not improve\n",
      "Epoch 464/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0151 - acc: 0.9980 - val_loss: 0.3042 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00464: val_loss did not improve\n",
      "Epoch 465/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0183 - acc: 0.9941 - val_loss: 0.3223 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00465: val_loss did not improve\n",
      "Epoch 466/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0086 - acc: 0.9971 - val_loss: 0.2705 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00466: val_loss did not improve\n",
      "Epoch 467/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0296 - acc: 0.9902 - val_loss: 0.2713 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00467: val_loss did not improve\n",
      "Epoch 468/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0166 - acc: 0.9951 - val_loss: 0.2705 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00468: val_loss did not improve\n",
      "Epoch 469/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0203 - acc: 0.9932 - val_loss: 0.2392 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00469: val_loss did not improve\n",
      "Epoch 470/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0183 - acc: 0.9951 - val_loss: 0.2445 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00470: val_loss did not improve\n",
      "Epoch 471/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0551 - acc: 0.9873 - val_loss: 0.2657 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00471: val_loss did not improve\n",
      "Epoch 472/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0109 - acc: 0.9961 - val_loss: 0.2974 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00472: val_loss did not improve\n",
      "Epoch 473/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0219 - acc: 0.9932 - val_loss: 0.2878 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00473: val_loss did not improve\n",
      "Epoch 474/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0245 - acc: 0.9941 - val_loss: 0.2896 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00474: val_loss did not improve\n",
      "Epoch 475/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0123 - acc: 0.9932 - val_loss: 0.3094 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00475: val_loss did not improve\n",
      "Epoch 476/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0309 - acc: 0.9932 - val_loss: 0.2914 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00476: val_loss did not improve\n",
      "Epoch 477/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0155 - acc: 0.9971 - val_loss: 0.3558 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00477: val_loss did not improve\n",
      "Epoch 478/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0187 - acc: 0.9932 - val_loss: 0.3500 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00478: val_loss did not improve\n",
      "Epoch 479/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0080 - acc: 0.9971 - val_loss: 0.3659 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00479: val_loss did not improve\n",
      "Epoch 480/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0209 - acc: 0.9932 - val_loss: 0.3367 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00480: val_loss did not improve\n",
      "Epoch 481/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0270 - acc: 0.9922 - val_loss: 0.2663 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00481: val_loss did not improve\n",
      "Epoch 482/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0158 - acc: 0.9951 - val_loss: 0.2791 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00482: val_loss did not improve\n",
      "Epoch 483/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0209 - acc: 0.9893 - val_loss: 0.2372 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00483: val_loss did not improve\n",
      "Epoch 484/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0121 - acc: 0.9951 - val_loss: 0.2370 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00484: val_loss did not improve\n",
      "Epoch 485/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0090 - acc: 0.9961 - val_loss: 0.2784 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00485: val_loss did not improve\n",
      "Epoch 486/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0086 - acc: 0.9971 - val_loss: 0.3445 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00486: val_loss did not improve\n",
      "Epoch 487/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0183 - acc: 0.9932 - val_loss: 0.3004 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00487: val_loss did not improve\n",
      "Epoch 488/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0186 - acc: 0.9941 - val_loss: 0.2920 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00488: val_loss did not improve\n",
      "Epoch 489/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0231 - acc: 0.9932 - val_loss: 0.3334 - val_acc: 0.9273\n",
      "\n",
      "Epoch 00489: val_loss did not improve\n",
      "Epoch 490/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0271 - acc: 0.9912 - val_loss: 0.2664 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00490: val_loss did not improve\n",
      "Epoch 491/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.3271 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00491: val_loss did not improve\n",
      "Epoch 492/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.2852 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00492: val_loss did not improve\n",
      "Epoch 493/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0104 - acc: 0.9990 - val_loss: 0.3135 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00493: val_loss did not improve\n",
      "Epoch 494/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0101 - acc: 0.9980 - val_loss: 0.2701 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00494: val_loss did not improve\n",
      "Epoch 495/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0221 - acc: 0.9951 - val_loss: 0.2755 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00495: val_loss did not improve\n",
      "Epoch 496/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.2305 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00496: val_loss did not improve\n",
      "Epoch 497/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0177 - acc: 0.9980 - val_loss: 0.2087 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00497: val_loss did not improve\n",
      "Epoch 498/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.2424 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00498: val_loss did not improve\n",
      "Epoch 499/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0108 - acc: 0.9980 - val_loss: 0.2728 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00499: val_loss did not improve\n",
      "Epoch 500/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.2759 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00500: val_loss did not improve\n",
      "Epoch 501/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0153 - acc: 0.9951 - val_loss: 0.2780 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00501: val_loss did not improve\n",
      "Epoch 502/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0052 - acc: 0.9990 - val_loss: 0.3267 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00502: val_loss did not improve\n",
      "Epoch 503/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.3572 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00503: val_loss did not improve\n",
      "Epoch 504/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0172 - acc: 0.9961 - val_loss: 0.3346 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00504: val_loss did not improve\n",
      "Epoch 505/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0096 - acc: 0.9951 - val_loss: 0.3242 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00505: val_loss did not improve\n",
      "Epoch 506/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0092 - acc: 0.9961 - val_loss: 0.3845 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00506: val_loss did not improve\n",
      "Epoch 507/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.4849 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00507: val_loss did not improve\n",
      "Epoch 508/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0138 - acc: 0.9961 - val_loss: 0.3549 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00508: val_loss did not improve\n",
      "Epoch 509/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0082 - acc: 0.9990 - val_loss: 0.3023 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00509: val_loss did not improve\n",
      "Epoch 510/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0044 - acc: 0.9980 - val_loss: 0.3994 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00510: val_loss did not improve\n",
      "Epoch 511/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.4429 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00511: val_loss did not improve\n",
      "Epoch 512/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.3320 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00512: val_loss did not improve\n",
      "Epoch 513/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0048 - acc: 0.9980 - val_loss: 0.4278 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00513: val_loss did not improve\n",
      "Epoch 514/1515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 49s 2s/step - loss: 0.0198 - acc: 0.9961 - val_loss: 0.3769 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00514: val_loss did not improve\n",
      "Epoch 515/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.3793 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00515: val_loss did not improve\n",
      "Epoch 516/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0078 - acc: 0.9961 - val_loss: 0.3185 - val_acc: 0.9211\n",
      "\n",
      "Epoch 00516: val_loss did not improve\n",
      "Epoch 517/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0147 - acc: 0.9941 - val_loss: 0.3685 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00517: val_loss did not improve\n",
      "Epoch 518/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0353 - acc: 0.9883 - val_loss: 0.3791 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00518: val_loss did not improve\n",
      "Epoch 519/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0198 - acc: 0.9922 - val_loss: 0.3069 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00519: val_loss did not improve\n",
      "Epoch 520/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0410 - acc: 0.9922 - val_loss: 0.2774 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00520: val_loss did not improve\n",
      "Epoch 521/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0066 - acc: 0.9990 - val_loss: 0.3430 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00521: val_loss did not improve\n",
      "Epoch 522/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0204 - acc: 0.9951 - val_loss: 0.3013 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00522: val_loss did not improve\n",
      "Epoch 523/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0079 - acc: 0.9980 - val_loss: 0.3507 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00523: val_loss did not improve\n",
      "Epoch 524/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0145 - acc: 0.9932 - val_loss: 0.3136 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00524: val_loss did not improve\n",
      "Epoch 525/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.2400 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00525: val_loss did not improve\n",
      "Epoch 526/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0106 - acc: 0.9961 - val_loss: 0.3360 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00526: val_loss did not improve\n",
      "Epoch 527/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0365 - acc: 0.9902 - val_loss: 0.2823 - val_acc: 0.9221\n",
      "\n",
      "Epoch 00527: val_loss did not improve\n",
      "Epoch 528/1515\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.0208 - acc: 0.9912 - val_loss: 0.2626 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00528: val_loss did not improve\n",
      "Epoch 529/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0107 - acc: 0.9971 - val_loss: 0.2449 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00529: val_loss did not improve\n",
      "Epoch 530/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0226 - acc: 0.9932 - val_loss: 0.2181 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00530: val_loss did not improve\n",
      "Epoch 531/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0103 - acc: 0.9990 - val_loss: 0.3128 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00531: val_loss did not improve\n",
      "Epoch 532/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0063 - acc: 0.9971 - val_loss: 0.2809 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00532: val_loss did not improve\n",
      "Epoch 533/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.2934 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00533: val_loss did not improve\n",
      "Epoch 534/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0118 - acc: 0.9971 - val_loss: 0.3046 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00534: val_loss did not improve\n",
      "Epoch 535/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0063 - acc: 0.9980 - val_loss: 0.3077 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00535: val_loss did not improve\n",
      "Epoch 536/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.2886 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00536: val_loss did not improve\n",
      "Epoch 537/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0217 - acc: 0.9951 - val_loss: 0.3131 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00537: val_loss did not improve\n",
      "Epoch 538/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0177 - acc: 0.9951 - val_loss: 0.3438 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00538: val_loss did not improve\n",
      "Epoch 539/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.3529 - val_acc: 0.9508\n",
      "\n",
      "Epoch 00539: val_loss did not improve\n",
      "Epoch 540/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0078 - acc: 0.9961 - val_loss: 0.2942 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00540: val_loss did not improve\n",
      "Epoch 541/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0157 - acc: 0.9951 - val_loss: 0.3898 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00541: val_loss did not improve\n",
      "Epoch 542/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0125 - acc: 0.9951 - val_loss: 0.3236 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00542: val_loss did not improve\n",
      "Epoch 543/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0209 - acc: 0.9941 - val_loss: 0.3578 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00543: val_loss did not improve\n",
      "Epoch 544/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0103 - acc: 0.9971 - val_loss: 0.2385 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00544: val_loss did not improve\n",
      "Epoch 545/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0190 - acc: 0.9951 - val_loss: 0.2598 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00545: val_loss did not improve\n",
      "Epoch 546/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0161 - acc: 0.9912 - val_loss: 0.2744 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00546: val_loss did not improve\n",
      "Epoch 547/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0102 - acc: 0.9961 - val_loss: 0.3730 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00547: val_loss did not improve\n",
      "Epoch 548/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0139 - acc: 0.9941 - val_loss: 0.3539 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00548: val_loss did not improve\n",
      "Epoch 549/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0255 - acc: 0.9951 - val_loss: 0.3455 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00549: val_loss did not improve\n",
      "Epoch 550/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0168 - acc: 0.9951 - val_loss: 0.2698 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00550: val_loss did not improve\n",
      "Epoch 551/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0143 - acc: 0.9961 - val_loss: 0.3358 - val_acc: 0.9597\n",
      "\n",
      "Epoch 00551: val_loss did not improve\n",
      "Epoch 552/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0150 - acc: 0.9971 - val_loss: 0.2807 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00552: val_loss did not improve\n",
      "Epoch 553/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.2632 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00553: val_loss did not improve\n",
      "Epoch 554/1515\n",
      "32/32 [==============================] - 50s 2s/step - loss: 0.0076 - acc: 0.9961 - val_loss: 0.2639 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00554: val_loss did not improve\n",
      "Epoch 555/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0048 - acc: 0.9980 - val_loss: 0.3123 - val_acc: 0.9447\n",
      "\n",
      "Epoch 00555: val_loss did not improve\n",
      "Epoch 556/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0249 - acc: 0.9941 - val_loss: 0.2550 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00556: val_loss did not improve\n",
      "Epoch 557/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0051 - acc: 0.9990 - val_loss: 0.2976 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00557: val_loss did not improve\n",
      "Epoch 558/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0080 - acc: 0.9961 - val_loss: 0.2869 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00558: val_loss did not improve\n",
      "Epoch 559/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0071 - acc: 0.9971 - val_loss: 0.3136 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00559: val_loss did not improve\n",
      "Epoch 560/1515\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.0095 - acc: 0.9990 - val_loss: 0.2672 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00560: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = 16188 #8094 #3036 #18046 #111589 #113243 #139987 \n",
    "nb_validation_samples= 336 \n",
    "epochs = int(nb_train_samples/batch_size)*3\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=batch_size, #nb_train_samples/batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=batch_size, #nb_validation_samples/batch_size, #val_batch_size,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03f5e406d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03f4122860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'],'r--')\n",
    "    plt.plot(history.history['val_loss'],'b-')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show();\n",
    "    plt.plot(history.history['acc'],'r--')\n",
    "    plt.plot(history.history['val_acc'],'b-')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.legend(['Train Acc', 'Valid Acc'])\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "\n",
    "# requires history=model.fit, fit_generator...\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = Loss: 0.136075, Acc@1: 0.95\n"
     ]
    }
   ],
   "source": [
    "#nb_test_samples = 168\n",
    "#steps = nb_test_samples/batch_size\n",
    "#scores = model.evaluate(X_test, y_test)\n",
    "scores = model.evaluate_generator(test_generator, steps=10, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(\"score = Loss: %f, Acc@1: %.2f\" % (scores[0],scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict_generator(test_generator, steps=10, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336, 2)\n",
      "(336, 2)\n"
     ]
    }
   ],
   "source": [
    "#print(test_generator.classes)\n",
    "steps = 11 #int(SIZE?/batch_size)\n",
    "preds = np.zeros((0,2))\n",
    "y_test = np.zeros((0,2))\n",
    "step_count = 0\n",
    "for batch_x, batch_y in test_generator:\n",
    "    if step_count < steps:\n",
    "        batch_preds = model.predict(batch_x)\n",
    "        #print(batch_preds.shape)\n",
    "        preds = np.vstack((preds,batch_preds))\n",
    "        #print(batch_y)\n",
    "        y_test = np.vstack((y_test,batch_y))\n",
    "        step_count = step_count + 1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(preds.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a48323198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_trues = [np.argmax(ii) for ii in y_test]\n",
    "y_preds = [np.argmax(ii) for ii in preds]\n",
    "\n",
    "# credit: https://tatwan.github.io/How-To-Plot-A-Confusion-Matrix-In-Python/    \n",
    "def plot_binary_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    classNames = classes\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=45)\n",
    "    plt.yticks(tick_marks, classNames)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "    \n",
    "cm = confusion_matrix(y_trues, y_preds)\n",
    "#print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "#print(tn, fp, fn, tp)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_binary_confusion_matrix(cm, classes=['Negative','Positive'], title='Confusion matrix, without normalization', cmap=plt.cm.Greens)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
